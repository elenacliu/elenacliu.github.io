
@misc{liu_rgbgrasp_2023,
	title = {{RGBGrasp}: {Image}-based {Object} {Grasping} by {Capturing} {Multiple} {Views} during {Robot} {Arm} {Movement} with {Neural} {Radiance} {Fields}},
	copyright = {All rights reserved},
	shorttitle = {{RGBGrasp}},
	url = {http://arxiv.org/abs/2311.16592},
	abstract = {Robotic research encounters a significant hurdle when it comes to the intricate task of grasping objects that come in various shapes, materials, and textures. Unlike many prior investigations that heavily leaned on specialized point-cloud cameras or abundant RGB visual data to gather 3D insights for object-grasping missions, this paper introduces a pioneering approach called RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D surroundings containing transparent and specular objects and achieve accurate grasping. Our method utilizes pre-trained depth prediction models to establish geometry constraints, enabling precise 3D structure estimation, even under limited view conditions. Finally, we integrate hash encoding and a proposal sampler strategy to significantly accelerate the 3D reconstruction process. These innovations significantly enhance the adaptability and effectiveness of our algorithm in real-world scenarios. Through comprehensive experimental validation, we demonstrate that RGBGrasp achieves remarkable success across a wide spectrum of object-grasping scenarios, establishing it as a promising solution for real-world robotic manipulation tasks. The demo of our method can be found on: https://sites.google.com/view/rgbgrasp},
	language = {en},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Liu, Chang and Shi, Kejian and Zhou, Kaichen and Wang, Haoxiao and Zhang, Jiyao and Dong, Hao},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16592 [cs]},
	keywords = {Computer Science - Robotics, /unread},
	file = {Liu et al. - 2023 - RGBGrasp Image-based Object Grasping by Capturing.pdf:files/4128/Liu et al. - 2023 - RGBGrasp Image-based Object Grasping by Capturing.pdf:application/pdf},
}

@inproceedings{yang_towards_2023,
	address = {Vancouver, BC, Canada},
	title = {Towards {Effective} {Adversarial} {Textured} {3D} {Meshes} on {Physical} {Face} {Recognition}},
	copyright = {All rights reserved},
	isbn = {9798350301298},
	url = {https://ieeexplore.ieee.org/document/10203114/},
	doi = {10.1109/CVPR52729.2023.00401},
	abstract = {Face recognition is a prevailing authentication solution in numerous biometric applications. Physical adversarial attacks, as an important surrogate, can identify the weaknesses of face recognition systems and evaluate their robustness before deployed. However, most existing physical attacks are either detectable readily or ineffective against commercial recognition systems. The goal of this work is to develop a more reliable technique that can carry out an endto-end evaluation of adversarial robustness for commercial systems. It requires that this technique can simultaneously deceive black-box recognition models and evade defensive mechanisms. To fulfill this, we design adversarial textured 3D meshes (AT3D) with an elaborate topology on a human face, which can be 3D-printed and pasted on the attackerâ€™s face to evade the defenses. However, the mesh-based optimization regime calculates gradients in high-dimensional mesh space, and can be trapped into local optima with unsatisfactory transferability. To deviate from the mesh-based space, we propose to perturb the low-dimensional coefficient space based on 3D Morphable Model, which significantly improves black-box transferability meanwhile enjoying faster search efficiency and better visual quality. Extensive experiments in digital and physical scenarios show that our method effectively explores the security vulnerabilities of multiple popular commercial services, including three recognition APIs, four anti-spoofing APIs, two prevailing mobile phones and two automated access control systems.},
	language = {en},
	urldate = {2023-11-29},
	booktitle = {2023 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Xiao and Liu, Chang and Xu, Longlong and Wang, Yikai and Dong, Yinpeng and Chen, Ning and Su, Hang and Zhu, Jun},
	month = jun,
	year = {2023},
	note = {CVPR2023Highlight},
	keywords = {/unread},
	pages = {4119--4128},
	file = {Yang et al. - 2023 - Towards Effective Adversarial Textured 3D Meshes o.pdf:files/4130/Yang et al. - 2023 - Towards Effective Adversarial Textured 3D Meshes o.pdf:application/pdf},
}

@inproceedings{ling2024articulated,
 abstract = {3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects.Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation. However, a significant challenge remains: while previous works use perfect point cloud generated in simulation, the models cannot directly apply to the noisy point cloud in the real-world. To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object. Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages. In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate. Then, we move the camera in front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions. The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicking real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem.},
 author = {Ling, Suhan and Wang, Yian and Wu, Shiguang and Zhuang, Yuzheng and Xu, Tianyi and Li, Yu and Liu, Chang and Dong, Hao},
 booktitle = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
 publisher = {IEEE},
 title = {Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise},
 year = {2024}
}
