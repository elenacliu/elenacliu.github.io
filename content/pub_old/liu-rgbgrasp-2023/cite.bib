@misc{liu_rgbgrasp_2023,
 abstract = {Robotic research encounters a significant hurdle when it comes to the intricate task of grasping objects that come in various shapes, materials, and textures. Unlike many prior investigations that heavily leaned on specialized point-cloud cameras or abundant RGB visual data to gather 3D insights for object-grasping missions, this paper introduces a pioneering approach called RGBGrasp. This method depends on a limited set of RGB views to perceive the 3D surroundings containing transparent and specular objects and achieve accurate grasping. Our method utilizes pre-trained depth prediction models to establish geometry constraints, enabling precise 3D structure estimation, even under limited view conditions. Finally, we integrate hash encoding and a proposal sampler strategy to significantly accelerate the 3D reconstruction process. These innovations significantly enhance the adaptability and effectiveness of our algorithm in real-world scenarios. Through comprehensive experimental validation, we demonstrate that RGBGrasp achieves remarkable success across a wide spectrum of object-grasping scenarios, establishing it as a promising solution for real-world robotic manipulation tasks. The demo of our method can be found on: https://sites.google.com/view/rgbgrasp},
 author = {Liu, Chang and Shi, Kejian and Zhou, Kaichen and Wang, Haoxiao and Zhang, Jiyao and Dong, Hao},
 copyright = {All rights reserved},
 file = {Liu et al. - 2023 - RGBGrasp Image-based Object Grasping by Capturing.pdf:files/4128/Liu et al. - 2023 - RGBGrasp Image-based Object Grasping by Capturing.pdf:application/pdf},
 keywords = {Computer Science - Robotics, /unread},
 language = {en},
 month = {November},
 note = {arXiv:2311.16592 [cs]},
 publisher = {arXiv},
 shorttitle = {RGBGrasp},
 title = {RGBGrasp: Image-based Object Grasping by Capturing Multiple Views during Robot Arm Movement with Neural Radiance Fields},
 url = {http://arxiv.org/abs/2311.16592},
 urldate = {2023-11-29},
 year = {2023}
}
